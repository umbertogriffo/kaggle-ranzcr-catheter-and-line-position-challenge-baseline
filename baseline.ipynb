{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "baseline",
      "provenance": [],
      "collapsed_sections": [
        "jTXksFtiHfDH",
        "pBETV6aO2d39",
        "oBEqkWIpH0zJ",
        "pRmp45qYY1NJ",
        "j3YbTNckY4ua"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umbertogriffo/kaggle-ranzcr-catheter-and-line-position-challenge-baseline/blob/master/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOf6f_dYMqoF"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTXksFtiHfDH"
      },
      "source": [
        "# Install Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMvFLpBQlBTR",
        "outputId": "eebb2f89-e1e1-4f44-b324-234dbf808b3e"
      },
      "source": [
        "!pip install kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle # force install the latest version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.10)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/33/365c0d13f07a2a54744d027fe20b60dacdfdfb33bc04746db6ad0b79340b/kaggle-1.5.10.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.10-cp37-none-any.whl size=73269 sha256=e5218c04684d2ed86dd63553240c7bb94fd7a870ce46a89b2c0cbc634c3c8027\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/d1/7e/6ce09b72b770149802c653a02783821629146983ee5a360f10\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.10\n",
            "    Uninstalling kaggle-1.5.10:\n",
            "      Successfully uninstalled kaggle-1.5.10\n",
            "Successfully installed kaggle-1.5.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBETV6aO2d39"
      },
      "source": [
        "# API Credentials\n",
        "\n",
        "To use the Kaggle API, sign up for a Kaggle account at https://www.kaggle.com. Then go to the 'Account' tab of your user profile (https://www.kaggle.com/username/account) and select 'Create API Token'. This will trigger the download of kaggle.json, a file containing your API credentials. \n",
        "\n",
        "Place this file on your Google Drive anywhere.\n",
        "\n",
        "With the next snippet you download your credentials to Colab and you can start using Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XCfsJyQs2uUo",
        "outputId": "2989053a-2012-4cba-bb27-6a8dea69151e"
      },
      "source": [
        "# Install gcloud python module and google-api-python-client module\n",
        "!pip install --upgrade gcloud\n",
        "!pip install --upgrade google-api-python-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gcloud\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/ab/d0cee58db2d8445c26e6f5db25d9b1f1aa14a3ab30eea8ce77ae808d10ef/gcloud-0.18.3.tar.gz (454kB)\n",
            "\r\u001b[K     |▊                               | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 11.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 7.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 81kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 102kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 122kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 133kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 143kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 153kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 163kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 174kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 184kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 194kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 204kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 215kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 225kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 235kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 245kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 256kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 266kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 276kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 286kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 296kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 307kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 317kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 327kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 337kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 348kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 358kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 368kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 378kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 389kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 399kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 409kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 419kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 430kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 440kB 6.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 450kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 460kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from gcloud) (0.17.4)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.7/dist-packages (from gcloud) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: oauth2client>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from gcloud) (4.1.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf!=3.0.0.b2.post1,>=3.0.0b2 in /usr/local/lib/python3.7/dist-packages (from gcloud) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from gcloud) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=2.0.1->gcloud) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=2.0.1->gcloud) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=2.0.1->gcloud) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.0.0.b2.post1,>=3.0.0b2->gcloud) (54.0.0)\n",
            "Building wheels for collected packages: gcloud\n",
            "  Building wheel for gcloud (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gcloud: filename=gcloud-0.18.3-cp37-none-any.whl size=602938 sha256=8acb39ef4f8ac95b72195afd418f83978542fd6c957c4957af63996c55e5e614\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/9b/9c/a01be401658fea33b93a35d03921b0c638266821b264dc8662\n",
            "Successfully built gcloud\n",
            "Installing collected packages: gcloud\n",
            "Successfully installed gcloud-0.18.3\n",
            "Collecting google-api-python-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/56/bc7314cd180d1420e4ef11202dc9548ec22237a0a6de1aaf37b460ee7753/google_api_python_client-2.0.2-py2.py3-none-any.whl (6.5MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-auth<2dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.27.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (0.0.4)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (0.17.4)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.15.0)\n",
            "Collecting google-api-core<2dev,>=1.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/1dfc9233b3cc2b3e2a1d2839e18f2ced8fe9586425aaa13483d47cef542b/google_api_core-1.26.1-py2.py3-none-any.whl (92kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (54.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2dev,>=1.16.0->google-api-python-client) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.10)\n",
            "Installing collected packages: google-api-core, google-api-python-client\n",
            "  Found existing installation: google-api-core 1.16.0\n",
            "    Uninstalling google-api-core-1.16.0:\n",
            "      Successfully uninstalled google-api-core-1.16.0\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed google-api-core-1.26.1 google-api-python-client-2.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "googleapiclient"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5_288BYp6H1",
        "outputId": "ced13066-7413-4f3b-e048-b61bec84f72c"
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYlMHoUHGaCx"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBEqkWIpH0zJ"
      },
      "source": [
        "# Download The Dataset From Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDyKzZPtOV1-",
        "outputId": "a66c4ac4-789a-4011-bf0d-cd611b4aa686"
      },
      "source": [
        "!kaggle competitions download -c ranzcr-clip-catheter-line-classification -p /content/\n",
        "!unzip -q /content/ranzcr-clip-catheter-line-classification.zip\n",
        "!rm /content/ranzcr-clip-catheter-line-classification.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ranzcr-clip-catheter-line-classification.zip to /content\n",
            "100% 11.7G/11.7G [03:09<00:00, 52.9MB/s]\n",
            "100% 11.7G/11.7G [03:09<00:00, 66.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyOzEerhOqsJ",
        "outputId": "feef634d-5f45-4eab-a778-17a4fc7b5029"
      },
      "source": [
        "%cd /content\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "adc.json\t       test\t       train_annotations.csv\n",
            "sample_data\t       test_tfrecords  train.csv\n",
            "sample_submission.csv  train\t       train_tfrecords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oggOAd7N_u_n"
      },
      "source": [
        "# Enabling and testing the TPU\r\n",
        "\r\n",
        "First, you'll need to enable TPUs for the notebook:\r\n",
        "\r\n",
        "- Navigate to Edit→Notebook Settings\r\n",
        "- select TPU from the Hardware Accelerator drop-down\r\n",
        "\r\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azlOYigdAHQW",
        "outputId": "784f590c-2c84-4d61-c494-ac9efaba82b1"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "print(\"Tensorflow version \" + tf.__version__)\r\n",
        "import os\r\n",
        "\r\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\r\n",
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n",
        "\r\n",
        "def auto_select_accelerator(tpu_worker_address=None):\r\n",
        "    try:\r\n",
        "        # tf.config.experimental_connect_to_host(tpu_worker_address)\r\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_worker_address) # TPU detection\r\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\r\n",
        "        print(\"Running on TPU:\", tpu.master())\r\n",
        "    except ValueError:\r\n",
        "        print('Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\r\n",
        "        strategy = tf.distribute.get_strategy()\r\n",
        "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\r\n",
        "    \r\n",
        "    return strategy\r\n",
        "\r\n",
        "strategy = auto_select_accelerator()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.92.21.58:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.92.21.58:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on TPU: grpc://10.92.21.58:8470\n",
            "Running on 8 replicas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub_StX_SAHne",
        "outputId": "1fcb70a2-6840-4d47-e8ec-52e000c55509"
      },
      "source": [
        "# Listing Devices\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 15866360561727782224, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14674281152\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 17166789748461145350\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8MgQBwwiAMWT",
        "outputId": "66c4f00a-8cef-4477-c5d0-9d85b55a2622"
      },
      "source": [
        "# Testing for GPU\r\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmp45qYY1NJ"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gj7cC2as8-6",
        "outputId": "d152d4a4-de11-4d05-f5b6-a9120635f45c"
      },
      "source": [
        "# http://scikit.ml/stratification.html\r\n",
        "# http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html#skmultilearn.model_selection.iterative_stratification.iterative_train_test_split\r\n",
        "!pip install scikit-multilearn\r\n",
        "!pip install arff\r\n",
        "# https://www.tensorflow.org/addons\r\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: arff in /usr/local/lib/python3.7/dist-packages (0.9)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufm6-06eCxnA"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gc\n",
        "import os\n",
        "from glob import glob\n",
        "from random import shuffle\n",
        "import cv2\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from albumentations import Compose, RandomRotate90, Transpose, Flip, OneOf, CLAHE, IAASharpen, IAAEmboss, RandomBrightnessContrast, JpegCompression, Blur, GaussNoise, HueSaturationValue, ShiftScaleRotate, Normalize\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from skmultilearn.model_selection import iterative_train_test_split, IterativeStratification\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import Xception, EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7, ResNet152V2\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, LearningRateScheduler\n",
        "from tensorflow.keras.layers import Lambda, Reshape, DepthwiseConv2D, ZeroPadding2D, Add, MaxPooling2D,Activation, Flatten, Conv2D, Dense, Input, Dropout, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg6uFuRPJBFX"
      },
      "source": [
        "For reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb5A1QoejOVy"
      },
      "source": [
        "SEED = 42\r\n",
        "\r\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "tf.random.set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3YbTNckY4ua"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FARFLMY4ISqM"
      },
      "source": [
        "## Build Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_7AdBHLCa8k"
      },
      "source": [
        "def build_decoder(target_size, with_labels=True, ext='jpg'):\r\n",
        "    def decode(path):\r\n",
        "        file_bytes = tf.io.read_file(path) # Reads and outputs the entire contents of the input filename.\r\n",
        "\r\n",
        "        if ext == 'png':\r\n",
        "            img = tf.image.decode_png(file_bytes, channels=3) # Decode a PNG-encoded image to a uint8 or uint16 tensor\r\n",
        "        elif ext in ['jpg', 'jpeg']:\r\n",
        "            img = tf.image.decode_jpeg(file_bytes, channels=3) # Decode a JPEG-encoded image to a uint8 tensor\r\n",
        "        else:\r\n",
        "            raise ValueError(\"Image extension not supported\")\r\n",
        "\r\n",
        "        img = tf.cast(img, tf.float32) / 255.0 # Casts a tensor to the type float32 and divides by 255.\r\n",
        "        img = tf.image.resize(img, target_size) # Resizing to target size\r\n",
        "        return img\r\n",
        "    \r\n",
        "    def decode_with_labels(path, label):\r\n",
        "        return decode(path), tf.dtypes.cast(label, tf.float32)\r\n",
        "    \r\n",
        "    return decode_with_labels if with_labels else decode\r\n",
        "\r\n",
        "def build_augmenter(with_labels=True):\r\n",
        "  def augment(img):\r\n",
        "      img = tf.image.random_flip_left_right(img)\r\n",
        "      img = tf.image.random_flip_up_down(img)\r\n",
        "      img = tf.image.random_saturation(img, 0.8, 1.2)\r\n",
        "      img = tf.image.random_brightness(img, 0.2)\r\n",
        "      img = tf.image.random_contrast(img, 0.8, 1.2)\r\n",
        "      img = tf.image.random_hue(img, 0.2)\r\n",
        "      return img\r\n",
        "  \r\n",
        "  def augment_with_labels(img, label):\r\n",
        "      return augment(img), label\r\n",
        "  \r\n",
        "  return augment_with_labels if with_labels else augment\r\n",
        "\r\n",
        "def build_dataset(paths, labels=None, bsize=32, cache=True,\r\n",
        "                  decode_fn=None, augment_fn=None,\r\n",
        "                  augment=True, repeat=True, shuffle=1024, \r\n",
        "                  cache_dir=\"\"):\r\n",
        "    if cache_dir != \"\" and cache is True:\r\n",
        "        os.makedirs(cache_dir, exist_ok=True)\r\n",
        "    \r\n",
        "    if decode_fn is None:\r\n",
        "        decode_fn = build_decoder(labels is not None)\r\n",
        "    \r\n",
        "    if augment_fn is None:\r\n",
        "        augment_fn = build_augmenter(labels is not None)\r\n",
        "    \r\n",
        "    AUTO = tf.data.experimental.AUTOTUNE\r\n",
        "    slices = paths if labels is None else (paths, labels)\r\n",
        "    \r\n",
        "    dset = tf.data.Dataset.from_tensor_slices(slices)\r\n",
        "    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\r\n",
        "    dset = dset.cache(cache_dir) if cache else dset\r\n",
        "    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\r\n",
        "    dset = dset.repeat() if repeat else dset\r\n",
        "    dset = dset.shuffle(shuffle) if shuffle else dset\r\n",
        "    dset = dset.batch(bsize).prefetch(AUTO) # overlaps data preprocessing and model execution while training\r\n",
        "    return dset\r\n",
        "\r\n",
        "def build_tta(paths, bsize=32, decode_fn = None, augment_fn = None, tta = False):\r\n",
        "  \r\n",
        "  AUTO = tf.data.experimental.AUTOTUNE\r\n",
        "  dset = tf.data.Dataset.from_tensor_slices(paths)\r\n",
        "  dset = dset.map(decode_fn, num_parallel_calls = AUTO)\r\n",
        "  dset = dset.map(augment_fn, num_parallel_calls = AUTO)\r\n",
        "  if tta:\r\n",
        "    dset = dset.repeat() \r\n",
        "  dset = dset.batch(bsize)\r\n",
        "  dset = dset.prefetch(AUTO)\r\n",
        "  return dset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPENwlyLJN-J"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQRpVRq4JNfx"
      },
      "source": [
        "def get_model(architecture, img_size, n_classes):\r\n",
        "  if architecture == \"Xception\":\r\n",
        "    net = Xception(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"ResNet152V2\":\r\n",
        "    net = ResNet152V2(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"EfficientNetB0\":\r\n",
        "    net = EfficientNetB0(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"EfficientNetB1\":\r\n",
        "    net = EfficientNetB1(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"EfficientNetB2\":\r\n",
        "    net = EfficientNetB2(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"EfficientNetB3\":\r\n",
        "    net = EfficientNetB3(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"EfficientNetB4\":\r\n",
        "    net = EfficientNetB4(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"EfficientNetB5\":\r\n",
        "    net = EfficientNetB5(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"EfficientNetB6\":\r\n",
        "    net = EfficientNetB6(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  elif architecture == \"EfficientNetB7\":\r\n",
        "    net = EfficientNetB7(include_top=False,input_shape=(img_size, img_size, 3), weights='imagenet')\r\n",
        "  else:\r\n",
        "     raise BaseException(f\"The architecture {architecture} is not supported.\")\r\n",
        "  x = net.output\r\n",
        "  x = GlobalAveragePooling2D()(x)\r\n",
        "  output = Dense(n_classes, activation='sigmoid')(x)\r\n",
        "  model = Model(inputs=net.input, outputs=output)\r\n",
        "\r\n",
        "  return model\r\n",
        "\r\n",
        "def get_callbacks(model_path, lrfn):\r\n",
        "\r\n",
        "  callbacks = []\r\n",
        "\r\n",
        "  callbacks.append(TerminateOnNaN())\r\n",
        "\r\n",
        "  callbacks.append(EarlyStopping(\r\n",
        "        monitor='val_loss',\r\n",
        "        patience=5,\r\n",
        "        mode='min',\r\n",
        "        verbose=1,\r\n",
        "        restore_best_weights=True))\r\n",
        "\r\n",
        "  callbacks.append(ModelCheckpoint(\r\n",
        "        model_path,\r\n",
        "        monitor='val_loss',\r\n",
        "        verbose=1,\r\n",
        "        save_best_only=True,\r\n",
        "        mode='min'))\r\n",
        "\r\n",
        "  callbacks.append(ReduceLROnPlateau(\r\n",
        "        monitor='val_loss', \r\n",
        "        factor=0.8, \r\n",
        "        patience=2, \r\n",
        "        verbose=1, \r\n",
        "        mode='auto', \r\n",
        "        epsilon=0.0001, \r\n",
        "        cooldown=5, \r\n",
        "        min_lr=0.00001))\r\n",
        "  \r\n",
        "  if lrfn is not None:\r\n",
        "    callbacks.append(LearningRateScheduler(\r\n",
        "          lambda epoch: lrfn(epoch), \r\n",
        "          verbose=True))\r\n",
        "\r\n",
        "  return callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtfnwYH8YlrY"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX7z2YdAHAX7"
      },
      "source": [
        "def plot_metric(history_in, metric_name, results_dir):\n",
        "    \"\"\"\n",
        "    Plot a metric of model's history.\n",
        "    \"\"\"\n",
        "\n",
        "    fig_acc = plt.figure(figsize=(10, 10))\n",
        "    plt.plot(history_in.history[metric_name])\n",
        "    plt.plot(history_in.history['val_' + metric_name])\n",
        "\n",
        "    plt.title('model ' + metric_name)\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    fig_acc.savefig(os.path.join(results_dir, \"model_\" + metric_name + \".png\"))\n",
        "\n",
        "    plt.cla()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def mkdir_if_not_exist(directory):\n",
        "\n",
        "        if not os.path.exists(directory):\n",
        "            try:\n",
        "                os.makedirs(directory)\n",
        "            except OSError as e:\n",
        "                print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QByTJjiEFGY"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igr2qySaFEAm"
      },
      "source": [
        "GCS_DS_PATH = \"gs://kds-8fdde4eb643ffca87f4d2e302b48cea859367cad0b9a6e98d65c407a\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7fApdJ2EKxE",
        "outputId": "b571365b-bca8-4278-832a-aa3ff0163e05"
      },
      "source": [
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "print(f\"{df.columns}\")\n",
        "print(len(df), df['StudyInstanceUID'].nunique())\n",
        "print('Number of Records:',len(df), 'Number of Patients:' ,df['PatientID'].nunique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['StudyInstanceUID', 'ETT - Abnormal', 'ETT - Borderline',\n",
            "       'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline',\n",
            "       'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal',\n",
            "       'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present',\n",
            "       'PatientID'],\n",
            "      dtype='object')\n",
            "30083 30083\n",
            "Number of Records: 30083 Number of Patients: 3255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Bi_Gsp7CERSt",
        "outputId": "566ab85e-f6fb-4f62-a70f-b6631da4f942"
      },
      "source": [
        "paths = GCS_DS_PATH + \"/train/\" + df['StudyInstanceUID'] + '.jpg'\r\n",
        "paths[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gs://kds-8fdde4eb643ffca87f4d2e302b48cea859367cad0b9a6e98d65c407a/train/1.2.826.0.1.3680043.8.498.26697628953273228189375557799582420561.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMVxLnlpEPlP",
        "outputId": "66c3678e-3647-49d1-b5e4-feca37fac9b4"
      },
      "source": [
        "# Get the multi-labels\n",
        "df_sub = pd.read_csv(\"/content/sample_submission.csv\")\n",
        "labels_cols = df_sub.columns[1:]\n",
        "n_classes = len(labels_cols)\n",
        "print(f\"{n_classes} - {labels_cols}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11 - Index(['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal',\n",
            "       'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal',\n",
            "       'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n",
            "       'Swan Ganz Catheter Present'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqtTMHh2Gotv",
        "outputId": "a4ed91a4-d9b0-4f26-ddb9-dd3b521d2381"
      },
      "source": [
        "labels = df[labels_cols].values\r\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlheqgW_EjAu"
      },
      "source": [
        "id_label_map = {k: v for k, v in zip(paths, labels)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWU8HV5SLkmp"
      },
      "source": [
        "#@title Configuration\r\n",
        "#@markdown \r\n",
        "train_limit = 30083  #@param {type: \"number\"}\r\n",
        "test_limit = 3582  #@param {type: \"number\"}\r\n",
        "img_size =  612#@param {type:\"integer\"}\r\n",
        "batch_size_in =   8#@param {type: \"integer\", min: 8, max: 128}\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oyW2FInLYoN"
      },
      "source": [
        "## Build train and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq5l8ZAfK63v",
        "outputId": "c4c16cc1-8b49-4eed-c31a-9ab2951d28fc"
      },
      "source": [
        "ndarr_paths = np.reshape(paths.to_numpy(), (paths.shape[0], 1))\r\n",
        "ndarr_paths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['gs://kds-8fdde4eb643ffca87f4d2e302b48cea859367cad0b9a6e98d65c407a/train/1.2.826.0.1.3680043.8.498.26697628953273228189375557799582420561.jpg'],\n",
              "       ['gs://kds-8fdde4eb643ffca87f4d2e302b48cea859367cad0b9a6e98d65c407a/train/1.2.826.0.1.3680043.8.498.46302891597398758759818628675365157729.jpg'],\n",
              "       ['gs://kds-8fdde4eb643ffca87f4d2e302b48cea859367cad0b9a6e98d65c407a/train/1.2.826.0.1.3680043.8.498.23819260719748494858948050424870692577.jpg'],\n",
              "       ...,\n",
              "       ['gs://kds-8fdde4eb643ffca87f4d2e302b48cea859367cad0b9a6e98d65c407a/train/1.2.826.0.1.3680043.8.498.43173270582850645437451931712017243531.jpg'],\n",
              "       ['gs://kds-8fdde4eb643ffca87f4d2e302b48cea859367cad0b9a6e98d65c407a/train/1.2.826.0.1.3680043.8.498.95092491950130838685690656747242898392.jpg'],\n",
              "       ['gs://kds-8fdde4eb643ffca87f4d2e302b48cea859367cad0b9a6e98d65c407a/train/1.2.826.0.1.3680043.8.498.99518162226171269731026325462883860316.jpg']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By7tRC_ILYHD",
        "outputId": "38b03b93-c2d6-45e4-988c-4a67dd4a8d1c"
      },
      "source": [
        "batch_size = strategy.num_replicas_in_sync * batch_size_in\r\n",
        "print(f\"Batch size: {batch_size}\")\r\n",
        "\r\n",
        "# train_paths, valid_paths, train_labels, valid_labels = train_test_split(paths[0:train_limit], labels[0:train_limit], test_size=0.1, shuffle=True, random_state=SEED)\r\n",
        "\r\n",
        "# Multi-label data stratification\r\n",
        "train_paths, train_labels, valid_paths, valid_labels = iterative_train_test_split(ndarr_paths[0:train_limit], labels[0:train_limit], test_size = 0.1)\r\n",
        "train_paths = train_paths.flatten()\r\n",
        "valid_paths = valid_paths.flatten()\r\n",
        "\r\n",
        "print(f\"# Training images: {train_paths.shape}\")\r\n",
        "print(f\"# Training labels: {train_labels.shape}\")\r\n",
        "print(f\"# Validation images: {valid_paths.shape}\")\r\n",
        "print(f\"# Validation labels: {valid_labels.shape}\")\r\n",
        "print(f\"# Training steps: {len(train_paths) // batch_size}\")\r\n",
        "print(f\"# Validation steps: {len(valid_paths) // batch_size}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size: 64\n",
            "# Training images: (27066,)\n",
            "# Training labels: (27066, 11)\n",
            "# Validation images: (3017,)\n",
            "# Validation labels: (3017, 11)\n",
            "# Training steps: 422\n",
            "# Validation steps: 47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spw89aX_LkKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6feee5a5-da23-4a59-fcfd-5b040b0626a7"
      },
      "source": [
        "# Build the tensorflow datasets\r\n",
        "decoder = build_decoder(target_size=(img_size, img_size), with_labels=True)\r\n",
        "\r\n",
        "# Build the tensorflow datasets\r\n",
        "dtrain = build_dataset(\r\n",
        "    train_paths, train_labels, bsize=batch_size, decode_fn=decoder\r\n",
        ")\r\n",
        "\r\n",
        "dvalid = build_dataset(\r\n",
        "    valid_paths, valid_labels, bsize=batch_size, \r\n",
        "    repeat=False, shuffle=False, augment=False, decode_fn=decoder\r\n",
        ")\r\n",
        "\r\n",
        "dtrain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 612, 612, 3), (None, 11)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ6zYP7PY9lV"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1b-EM9fm4tg",
        "cellView": "form"
      },
      "source": [
        "#@title Configuration\n",
        "#@markdown \n",
        "output_path = \"model\" #@param {type: \"string\"}\n",
        "epochs = 30#@param {type:\"integer\"}\n",
        "n_fold = 5  #@param {type: \"slider\", min: 2, max: 10}\n",
        "architecture = \"EfficientNetB5\" #@param [\"Xception\", \"ResNet152V2\", \"EfficientNetB0\", \"EfficientNetB1\", \"EfficientNetB2\", \"EfficientNetB3\", \"EfficientNetB4\", \"EfficientNetB5\", \"EfficientNetB6\", \"EfficientNetB7\"]\n",
        "use_lr_strategy = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auCH1CVHmZPt"
      },
      "source": [
        "Calculate and show a learning rate schedule. We start with a fairly low rate, as we're using a pre-trained model and don't want to undo all the fine work put into training it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "36kaUFfDT_i3",
        "outputId": "a8cc9dba-df7d-4bd3-baf2-c7fafc0d10ac"
      },
      "source": [
        "start_lr = 0.00001\r\n",
        "min_lr = 0.00001\r\n",
        "max_lr = 0.00005 * strategy.num_replicas_in_sync\r\n",
        "rampup_epochs = 5\r\n",
        "sustain_epochs = 0\r\n",
        "exp_decay = .8\r\n",
        "\r\n",
        "def lrfn(epoch):\r\n",
        "  if epoch < rampup_epochs:\r\n",
        "    return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\r\n",
        "  elif epoch < rampup_epochs + sustain_epochs:\r\n",
        "    return max_lr\r\n",
        "  else:\r\n",
        "    return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\r\n",
        "    \r\n",
        "rang = np.arange(epochs)\r\n",
        "y = [lrfn(x) for x in rang]\r\n",
        "plt.plot(rang, y)\r\n",
        "print('Learning rate per epoch:')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate per epoch:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwp8FfHZ1L0T"
      },
      "source": [
        "### Single model\r\n",
        "To get maxmimum accuracy, we leverage a pretrained image recognition model. We drop the ImageNet-specific top layers (`include_top=false`), and add a max pooling and a softmax layer to predict our 11 classes.\r\n",
        "We also use [Focal loss](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy) since we have highly imbalanced classes. It down-weights well-classified examples and focuses on hard examples. The loss value is much high for a sample which is misclassified by the classifier as compared to the loss value corresponding to a well-classified example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlrxrAFQ22JG"
      },
      "source": [
        "with strategy.scope():\n",
        "  model = get_model(architecture, img_size, n_classes)\n",
        "  model.compile(tf.optimizers.Adam(lr=1e-3),\n",
        "                loss=tfa.losses.SigmoidFocalCrossEntropy(alpha = 0.50, gamma = 2.0),\n",
        "                metrics=[tf.keras.metrics.AUC(name=\"auc\", multi_label=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D-lSKfP9JXG",
        "outputId": "885cb798-0c1e-4468-eb56-6002a6448542"
      },
      "source": [
        "path = os.path.join(\"output_path\", \"single_model\")\n",
        "mkdir_if_not_exist(path)\n",
        "model_path = os.path.join(path, f\"{architecture}_model.h5\")\n",
        "\n",
        "lr_strategy=lrfn if use_lr_strategy else None\n",
        "callbacks = get_callbacks(model_path, lr_strategy)\n",
        "\n",
        "with strategy.scope():\n",
        "  history = model.fit(\n",
        "          dtrain,\n",
        "          validation_data=dvalid,\n",
        "          epochs=epochs, \n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          steps_per_epoch=len(train_paths) // batch_size,\n",
        "          validation_steps=len(valid_paths) // batch_size)\n",
        "\n",
        "  # summarize history\n",
        "  plot_metric(history, 'loss', path)\n",
        "  plot_metric(history, 'auc', path)\n",
        "\n",
        "# To avoid timeout \n",
        "print(\"Avoiding timeout...\")\n",
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "422/422 [==============================] - 1335s 3s/step - loss: 0.4043 - auc: 0.7035 - val_loss: 0.4376 - val_auc: 0.7058\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.43761, saving model to output_path/single_model/EfficientNetB5_model.h5\n",
            "Epoch 2/30\n",
            "422/422 [==============================] - 329s 780ms/step - loss: 0.3475 - auc: 0.7771 - val_loss: 0.4890 - val_auc: 0.5537\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.43761\n",
            "Epoch 3/30\n",
            "422/422 [==============================] - 330s 782ms/step - loss: 0.3353 - auc: 0.7966 - val_loss: 0.6460 - val_auc: 0.4986\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.43761\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "Epoch 4/30\n",
            "400/422 [===========================>..] - ETA: 16s - loss: 0.3252 - auc: 0.8095"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xJj9CAJ_G-ru",
        "outputId": "8e499b6b-c08d-4b54-d6fe-37518faefb0f"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.download(model_path) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_05683be2-0631-44f1-b171-23eebdc2ace7\", \"Xception_model.h5\", 250928616)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqqEKrAvtfgc"
      },
      "source": [
        "### Single model predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oQpLJ8sZmN"
      },
      "source": [
        "with strategy.scope():\r\n",
        "    model = tf.keras.models.load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfs3FFTjs6b9",
        "outputId": "bc32b0de-f858-422c-983f-a55cee085481"
      },
      "source": [
        "test_decoder = build_decoder(with_labels=False, target_size=(img_size, img_size))\r\n",
        "\r\n",
        "df_sub = pd.read_csv(\"/content/sample_submission.csv\")\r\n",
        "test_paths = GCS_DS_PATH + \"/test/\" + df_sub['StudyInstanceUID'] + '.jpg'\r\n",
        "\r\n",
        "dtest = build_dataset(\r\n",
        "    test_paths, bsize=batch_size, repeat=False, \r\n",
        "    shuffle=False, augment=False, cache=False, \r\n",
        "    decode_fn=test_decoder\r\n",
        ")\r\n",
        "dtest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (None, 768, 768, 3), types: tf.float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25qF-JULtk3L",
        "outputId": "4fe29717-5d3a-4629-e1f1-e477cf0dbd4f"
      },
      "source": [
        "y_preds = model.predict(dtest, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "448/448 [==============================] - 119s 262ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "EXjJSTzjtpEs",
        "outputId": "4f230c9c-77b2-4c73-f09c-cb0f0d8962f6"
      },
      "source": [
        "df_sub.iloc[:, 1:] = y_preds\r\n",
        "display(df_sub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudyInstanceUID</th>\n",
              "      <th>ETT - Abnormal</th>\n",
              "      <th>ETT - Borderline</th>\n",
              "      <th>ETT - Normal</th>\n",
              "      <th>NGT - Abnormal</th>\n",
              "      <th>NGT - Borderline</th>\n",
              "      <th>NGT - Incompletely Imaged</th>\n",
              "      <th>NGT - Normal</th>\n",
              "      <th>CVC - Abnormal</th>\n",
              "      <th>CVC - Borderline</th>\n",
              "      <th>CVC - Normal</th>\n",
              "      <th>Swan Ganz Catheter Present</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.46923145579096002617...</td>\n",
              "      <td>0.163448</td>\n",
              "      <td>0.480167</td>\n",
              "      <td>0.503715</td>\n",
              "      <td>0.019151</td>\n",
              "      <td>0.050424</td>\n",
              "      <td>0.107511</td>\n",
              "      <td>0.920570</td>\n",
              "      <td>0.110259</td>\n",
              "      <td>0.283620</td>\n",
              "      <td>0.842426</td>\n",
              "      <td>0.991603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.84006870182611080091...</td>\n",
              "      <td>0.017474</td>\n",
              "      <td>0.017323</td>\n",
              "      <td>0.029714</td>\n",
              "      <td>0.039311</td>\n",
              "      <td>0.024697</td>\n",
              "      <td>0.066573</td>\n",
              "      <td>0.031252</td>\n",
              "      <td>0.060093</td>\n",
              "      <td>0.148542</td>\n",
              "      <td>0.891637</td>\n",
              "      <td>0.004435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12219033294413119947...</td>\n",
              "      <td>0.013433</td>\n",
              "      <td>0.012706</td>\n",
              "      <td>0.055000</td>\n",
              "      <td>0.143001</td>\n",
              "      <td>0.073421</td>\n",
              "      <td>0.194078</td>\n",
              "      <td>0.068333</td>\n",
              "      <td>0.156495</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.488326</td>\n",
              "      <td>0.016742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.84994474380235968109...</td>\n",
              "      <td>0.091283</td>\n",
              "      <td>0.191112</td>\n",
              "      <td>0.229841</td>\n",
              "      <td>0.185928</td>\n",
              "      <td>0.066009</td>\n",
              "      <td>0.814516</td>\n",
              "      <td>0.165254</td>\n",
              "      <td>0.189049</td>\n",
              "      <td>0.333241</td>\n",
              "      <td>0.777059</td>\n",
              "      <td>0.332558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.35798987793805669662...</td>\n",
              "      <td>0.094455</td>\n",
              "      <td>0.078671</td>\n",
              "      <td>0.036008</td>\n",
              "      <td>0.100051</td>\n",
              "      <td>0.065798</td>\n",
              "      <td>0.044584</td>\n",
              "      <td>0.203545</td>\n",
              "      <td>0.071929</td>\n",
              "      <td>0.222991</td>\n",
              "      <td>0.843799</td>\n",
              "      <td>0.013340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3577</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.81464483108873296584...</td>\n",
              "      <td>0.072672</td>\n",
              "      <td>0.123288</td>\n",
              "      <td>0.127274</td>\n",
              "      <td>0.117141</td>\n",
              "      <td>0.067327</td>\n",
              "      <td>0.846071</td>\n",
              "      <td>0.196342</td>\n",
              "      <td>0.116550</td>\n",
              "      <td>0.273004</td>\n",
              "      <td>0.760274</td>\n",
              "      <td>0.187562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3578</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.33579133018211530710...</td>\n",
              "      <td>0.058521</td>\n",
              "      <td>0.048267</td>\n",
              "      <td>0.051908</td>\n",
              "      <td>0.088657</td>\n",
              "      <td>0.079469</td>\n",
              "      <td>0.047833</td>\n",
              "      <td>0.196794</td>\n",
              "      <td>0.421112</td>\n",
              "      <td>0.461826</td>\n",
              "      <td>0.461826</td>\n",
              "      <td>0.141738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3579</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.61472811086105902907...</td>\n",
              "      <td>0.027224</td>\n",
              "      <td>0.039311</td>\n",
              "      <td>0.036949</td>\n",
              "      <td>0.035518</td>\n",
              "      <td>0.044070</td>\n",
              "      <td>0.008125</td>\n",
              "      <td>0.065572</td>\n",
              "      <td>0.092413</td>\n",
              "      <td>0.280129</td>\n",
              "      <td>0.760023</td>\n",
              "      <td>0.005993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3580</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.19434375795525494655...</td>\n",
              "      <td>0.012600</td>\n",
              "      <td>0.018724</td>\n",
              "      <td>0.052611</td>\n",
              "      <td>0.023162</td>\n",
              "      <td>0.044503</td>\n",
              "      <td>0.017956</td>\n",
              "      <td>0.110142</td>\n",
              "      <td>0.211471</td>\n",
              "      <td>0.575033</td>\n",
              "      <td>0.303013</td>\n",
              "      <td>0.018758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3581</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.21182456828145534541...</td>\n",
              "      <td>0.031076</td>\n",
              "      <td>0.038339</td>\n",
              "      <td>0.041228</td>\n",
              "      <td>0.036193</td>\n",
              "      <td>0.017785</td>\n",
              "      <td>0.118551</td>\n",
              "      <td>0.023882</td>\n",
              "      <td>0.096354</td>\n",
              "      <td>0.200444</td>\n",
              "      <td>0.835382</td>\n",
              "      <td>0.015071</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3582 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       StudyInstanceUID  ...  Swan Ganz Catheter Present\n",
              "0     1.2.826.0.1.3680043.8.498.46923145579096002617...  ...                    0.991603\n",
              "1     1.2.826.0.1.3680043.8.498.84006870182611080091...  ...                    0.004435\n",
              "2     1.2.826.0.1.3680043.8.498.12219033294413119947...  ...                    0.016742\n",
              "3     1.2.826.0.1.3680043.8.498.84994474380235968109...  ...                    0.332558\n",
              "4     1.2.826.0.1.3680043.8.498.35798987793805669662...  ...                    0.013340\n",
              "...                                                 ...  ...                         ...\n",
              "3577  1.2.826.0.1.3680043.8.498.81464483108873296584...  ...                    0.187562\n",
              "3578  1.2.826.0.1.3680043.8.498.33579133018211530710...  ...                    0.141738\n",
              "3579  1.2.826.0.1.3680043.8.498.61472811086105902907...  ...                    0.005993\n",
              "3580  1.2.826.0.1.3680043.8.498.19434375795525494655...  ...                    0.018758\n",
              "3581  1.2.826.0.1.3680043.8.498.21182456828145534541...  ...                    0.015071\n",
              "\n",
              "[3582 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vkbTSFztsBM"
      },
      "source": [
        "df_sub.to_csv('submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C68h06shyB5z"
      },
      "source": [
        "#### TTA\r\n",
        "A final technique that can raise accuracy by one percent or two is test time augmentation (TTA). This involves taking a series of different versions of the original image (for example cropping different areas, or changing the zoom) and passing them through the model. The average output is then calculated for the different versions and this is given as the final output score for the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6hNj7FOyqvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "aa0f3510-239e-49af-e86a-f1ec001b11e4"
      },
      "source": [
        "# Test time augmentation rounds\r\n",
        "TTA = 5\r\n",
        "NUM_TESTING_IMAGES = df_sub.shape[0]\r\n",
        "steps = TTA * ((NUM_TESTING_IMAGES / batch_size) + 1)\r\n",
        "\r\n",
        "# Get the test dataset with tta to extract image\r\n",
        "tta_dtest = build_tta(test_paths,\r\n",
        "                      decode_fn=build_decoder(with_labels=False, \r\n",
        "                                              target_size=(img_size, img_size)),\r\n",
        "                      augment_fn=build_augmenter(with_labels=False), \r\n",
        "                      bsize=batch_size, \r\n",
        "                      tta = True)\r\n",
        "\r\n",
        "probabilities = model.predict(tta_dtest, steps = steps, verbose=1)[: TTA * NUM_TESTING_IMAGES]\r\n",
        "probabilities = np.mean(probabilities.reshape((NUM_TESTING_IMAGES, TTA, n_classes), order = 'F'), axis = 1)\r\n",
        "\r\n",
        "df_sub.iloc[:, 1:] = probabilities\r\n",
        "display(df_sub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2243/2243 [==============================] - 342s 152ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudyInstanceUID</th>\n",
              "      <th>ETT - Abnormal</th>\n",
              "      <th>ETT - Borderline</th>\n",
              "      <th>ETT - Normal</th>\n",
              "      <th>NGT - Abnormal</th>\n",
              "      <th>NGT - Borderline</th>\n",
              "      <th>NGT - Incompletely Imaged</th>\n",
              "      <th>NGT - Normal</th>\n",
              "      <th>CVC - Abnormal</th>\n",
              "      <th>CVC - Borderline</th>\n",
              "      <th>CVC - Normal</th>\n",
              "      <th>Swan Ganz Catheter Present</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.46923145579096002617...</td>\n",
              "      <td>0.203044</td>\n",
              "      <td>0.470929</td>\n",
              "      <td>0.502128</td>\n",
              "      <td>0.037043</td>\n",
              "      <td>0.076248</td>\n",
              "      <td>0.096485</td>\n",
              "      <td>0.895558</td>\n",
              "      <td>0.179277</td>\n",
              "      <td>0.334679</td>\n",
              "      <td>0.758563</td>\n",
              "      <td>0.945995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.84006870182611080091...</td>\n",
              "      <td>0.018607</td>\n",
              "      <td>0.018920</td>\n",
              "      <td>0.028016</td>\n",
              "      <td>0.033905</td>\n",
              "      <td>0.018180</td>\n",
              "      <td>0.070100</td>\n",
              "      <td>0.024521</td>\n",
              "      <td>0.080314</td>\n",
              "      <td>0.181349</td>\n",
              "      <td>0.851251</td>\n",
              "      <td>0.006539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12219033294413119947...</td>\n",
              "      <td>0.015097</td>\n",
              "      <td>0.013354</td>\n",
              "      <td>0.048031</td>\n",
              "      <td>0.109525</td>\n",
              "      <td>0.059120</td>\n",
              "      <td>0.136682</td>\n",
              "      <td>0.063098</td>\n",
              "      <td>0.165148</td>\n",
              "      <td>0.451397</td>\n",
              "      <td>0.492348</td>\n",
              "      <td>0.016332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.84994474380235968109...</td>\n",
              "      <td>0.110147</td>\n",
              "      <td>0.209086</td>\n",
              "      <td>0.265355</td>\n",
              "      <td>0.179103</td>\n",
              "      <td>0.088013</td>\n",
              "      <td>0.803160</td>\n",
              "      <td>0.182418</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.334931</td>\n",
              "      <td>0.686372</td>\n",
              "      <td>0.284333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.35798987793805669662...</td>\n",
              "      <td>0.066202</td>\n",
              "      <td>0.059943</td>\n",
              "      <td>0.034095</td>\n",
              "      <td>0.062333</td>\n",
              "      <td>0.048361</td>\n",
              "      <td>0.035568</td>\n",
              "      <td>0.278521</td>\n",
              "      <td>0.084069</td>\n",
              "      <td>0.219709</td>\n",
              "      <td>0.813436</td>\n",
              "      <td>0.014355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3577</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.81464483108873296584...</td>\n",
              "      <td>0.050990</td>\n",
              "      <td>0.029446</td>\n",
              "      <td>0.040025</td>\n",
              "      <td>0.123444</td>\n",
              "      <td>0.069599</td>\n",
              "      <td>0.807612</td>\n",
              "      <td>0.233267</td>\n",
              "      <td>0.117596</td>\n",
              "      <td>0.229538</td>\n",
              "      <td>0.768369</td>\n",
              "      <td>0.049085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3578</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.33579133018211530710...</td>\n",
              "      <td>0.044094</td>\n",
              "      <td>0.031430</td>\n",
              "      <td>0.038638</td>\n",
              "      <td>0.081589</td>\n",
              "      <td>0.066154</td>\n",
              "      <td>0.031451</td>\n",
              "      <td>0.169563</td>\n",
              "      <td>0.385387</td>\n",
              "      <td>0.459632</td>\n",
              "      <td>0.471737</td>\n",
              "      <td>0.102210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3579</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.61472811086105902907...</td>\n",
              "      <td>0.021058</td>\n",
              "      <td>0.030765</td>\n",
              "      <td>0.038075</td>\n",
              "      <td>0.029219</td>\n",
              "      <td>0.034207</td>\n",
              "      <td>0.005748</td>\n",
              "      <td>0.061859</td>\n",
              "      <td>0.084614</td>\n",
              "      <td>0.301396</td>\n",
              "      <td>0.711508</td>\n",
              "      <td>0.004996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3580</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.19434375795525494655...</td>\n",
              "      <td>0.014535</td>\n",
              "      <td>0.023680</td>\n",
              "      <td>0.052516</td>\n",
              "      <td>0.030026</td>\n",
              "      <td>0.046622</td>\n",
              "      <td>0.019507</td>\n",
              "      <td>0.134263</td>\n",
              "      <td>0.215323</td>\n",
              "      <td>0.636493</td>\n",
              "      <td>0.267537</td>\n",
              "      <td>0.013966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3581</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.21182456828145534541...</td>\n",
              "      <td>0.032753</td>\n",
              "      <td>0.038067</td>\n",
              "      <td>0.060150</td>\n",
              "      <td>0.042910</td>\n",
              "      <td>0.019758</td>\n",
              "      <td>0.114271</td>\n",
              "      <td>0.032237</td>\n",
              "      <td>0.151810</td>\n",
              "      <td>0.240421</td>\n",
              "      <td>0.760480</td>\n",
              "      <td>0.018231</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3582 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       StudyInstanceUID  ...  Swan Ganz Catheter Present\n",
              "0     1.2.826.0.1.3680043.8.498.46923145579096002617...  ...                    0.945995\n",
              "1     1.2.826.0.1.3680043.8.498.84006870182611080091...  ...                    0.006539\n",
              "2     1.2.826.0.1.3680043.8.498.12219033294413119947...  ...                    0.016332\n",
              "3     1.2.826.0.1.3680043.8.498.84994474380235968109...  ...                    0.284333\n",
              "4     1.2.826.0.1.3680043.8.498.35798987793805669662...  ...                    0.014355\n",
              "...                                                 ...  ...                         ...\n",
              "3577  1.2.826.0.1.3680043.8.498.81464483108873296584...  ...                    0.049085\n",
              "3578  1.2.826.0.1.3680043.8.498.33579133018211530710...  ...                    0.102210\n",
              "3579  1.2.826.0.1.3680043.8.498.61472811086105902907...  ...                    0.004996\n",
              "3580  1.2.826.0.1.3680043.8.498.19434375795525494655...  ...                    0.013966\n",
              "3581  1.2.826.0.1.3680043.8.498.21182456828145534541...  ...                    0.018231\n",
              "\n",
              "[3582 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1caPfOOFn5r"
      },
      "source": [
        "df_sub.to_csv('submission_tta.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ye4Hxtx3fE"
      },
      "source": [
        "### Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xkrs_gE-Du6M",
        "outputId": "4db84b21-f680-4bf7-a8fb-a4402c466c9f"
      },
      "source": [
        "image_paths = paths[0:train_limit]\r\n",
        "image_labels = labels[0:train_limit]\r\n",
        "\r\n",
        "kf = KFold(n_splits=n_fold)\r\n",
        "\r\n",
        "for fold in range(0, n_fold):\r\n",
        "\r\n",
        "    print(f\"\\nFold: {fold}\")\r\n",
        "\r\n",
        "    path = os.path.join(\"output_path\", str(fold))\r\n",
        "    mkdir_if_not_exist(path)\r\n",
        "\r\n",
        "    model_path = os.path.join(path, \"model_\"+str(fold)+\".h5\")\r\n",
        "\r\n",
        "    result = next(kf.split(X=image_paths, y=image_labels), None)\r\n",
        "\r\n",
        "    train = image_paths[result[0]].tolist()\r\n",
        "    train_labels = [id_label_map[k] for k in train]\r\n",
        "    val = image_paths[result[1]].tolist()\r\n",
        "    valid_labels = [id_label_map[k] for k in val]\r\n",
        "\r\n",
        "    print(f\"train shape: {len(train)}\")\r\n",
        "    print(f\"train steps_per_epoch: {len(train) // batch_size}\")\r\n",
        "    print(f\"val shape: {len(val)}\")\r\n",
        "    print(f\"validation steps_per_epoch: {len(val) // batch_size}\")\r\n",
        "\r\n",
        "    # Build the tensorflow datasets\r\n",
        "    dtrain = build_dataset(\r\n",
        "        train, train_labels, bsize=batch_size, decode_fn=decoder\r\n",
        "    )\r\n",
        "\r\n",
        "    dvalid = build_dataset(\r\n",
        "        val, valid_labels, bsize=batch_size, \r\n",
        "        repeat=False, shuffle=False, augment=False, decode_fn=decoder\r\n",
        "    )\r\n",
        "\r\n",
        "    tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "    callbacks = get_callbacks(model_path, lrfn)\r\n",
        "\r\n",
        "    with strategy.scope():\r\n",
        "      model = get_model(architecture, img_size, n_classes)\r\n",
        "      model.compile(tf.optimizers.Adam(lr=1e-4),\r\n",
        "                    loss='binary_crossentropy',\r\n",
        "                    metrics=[tf.keras.metrics.AUC(name=\"auc\", multi_label=True)])\r\n",
        "      \r\n",
        "      ###########\r\n",
        "      # Training\r\n",
        "      ###########\r\n",
        "\r\n",
        "      history = model.fit(\r\n",
        "          dtrain,\r\n",
        "          validation_data=dvalid,\r\n",
        "          epochs=epochs, verbose=1,\r\n",
        "          callbacks=callbacks,\r\n",
        "          steps_per_epoch=len(train) // batch_size,\r\n",
        "          validation_steps=len(val) // batch_size)\r\n",
        "\r\n",
        "      # summarize history\r\n",
        "      plot_metric(history, 'loss', path)\r\n",
        "      plot_metric(history, 'auc', path)\r\n",
        "      \r\n",
        "      # in order to release the memory\r\n",
        "      del history\r\n",
        "      del model\r\n",
        "      gc.collect()\r\n",
        "      # tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "# To avoid timeout \r\n",
        "print(\"Avoiding timeout...\")\r\n",
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold: 0\n",
            "train shape: 24066\n",
            "train steps_per_epoch: 376\n",
            "val shape: 6017\n",
            "validation steps_per_epoch: 94\n",
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "376/376 [==============================] - 664s 2s/step - loss: 0.7018 - auc: 0.5364 - val_loss: 0.5757 - val_auc: 0.4758\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.57572, saving model to output_path/0/model_0.h5\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 8.8e-05.\n",
            "376/376 [==============================] - 105s 279ms/step - loss: 0.3912 - auc: 0.5882 - val_loss: 0.3161 - val_auc: 0.7144\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.57572 to 0.31614, saving model to output_path/0/model_0.h5\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000166.\n",
            "376/376 [==============================] - 105s 279ms/step - loss: 0.2822 - auc: 0.6909 - val_loss: 0.2574 - val_auc: 0.7842\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.31614 to 0.25736, saving model to output_path/0/model_0.h5\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.000244.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.2476 - auc: 0.7380 - val_loss: 0.2308 - val_auc: 0.8128\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.25736 to 0.23082, saving model to output_path/0/model_0.h5\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.2285 - auc: 0.7856 - val_loss: 0.2051 - val_auc: 0.8328\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.23082 to 0.20514, saving model to output_path/0/model_0.h5\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.2122 - auc: 0.8149 - val_loss: 0.2129 - val_auc: 0.8503\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.20514\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 105s 279ms/step - loss: 0.1947 - auc: 0.8558 - val_loss: 0.1839 - val_auc: 0.8849\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.20514 to 0.18389, saving model to output_path/0/model_0.h5\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.1800 - auc: 0.8824 - val_loss: 0.1735 - val_auc: 0.9043\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.18389 to 0.17352, saving model to output_path/0/model_0.h5\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1659 - auc: 0.9039 - val_loss: 0.1699 - val_auc: 0.8947\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.17352 to 0.16991, saving model to output_path/0/model_0.h5\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.1528 - auc: 0.9207 - val_loss: 0.1623 - val_auc: 0.9162\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.16991 to 0.16232, saving model to output_path/0/model_0.h5\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.1401 - auc: 0.9363 - val_loss: 0.1647 - val_auc: 0.9206\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.16232\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.1298 - auc: 0.9421 - val_loss: 0.1588 - val_auc: 0.9222\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.16232 to 0.15880, saving model to output_path/0/model_0.h5\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1188 - auc: 0.9544 - val_loss: 0.1677 - val_auc: 0.9145\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.15880\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1082 - auc: 0.9580 - val_loss: 0.1682 - val_auc: 0.9121\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.15880\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.0344912344589834e-05.\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.0992 - auc: 0.9683 - val_loss: 0.1710 - val_auc: 0.9130\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.15880\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.0858 - auc: 0.9765 - val_loss: 0.1730 - val_auc: 0.9139\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.15880\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 4.3500744908800015e-05.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.0834 - auc: 0.9733 - val_loss: 0.1830 - val_auc: 0.9139\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.15880\n",
            "Epoch 00017: early stopping\n",
            "\n",
            "Fold: 1\n",
            "train shape: 24066\n",
            "train steps_per_epoch: 376\n",
            "val shape: 6017\n",
            "validation steps_per_epoch: 94\n",
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "376/376 [==============================] - 678s 2s/step - loss: 0.7380 - auc: 0.5402 - val_loss: 0.5407 - val_auc: 0.5909\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.54065, saving model to output_path/1/model_1.h5\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 8.8e-05.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.3744 - auc: 0.5800 - val_loss: 0.2780 - val_auc: 0.7533\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.54065 to 0.27800, saving model to output_path/1/model_1.h5\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000166.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.2831 - auc: 0.6762 - val_loss: 0.2360 - val_auc: 0.7827\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.27800 to 0.23601, saving model to output_path/1/model_1.h5\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.000244.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.2438 - auc: 0.7333 - val_loss: 0.2140 - val_auc: 0.8167\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.23601 to 0.21401, saving model to output_path/1/model_1.h5\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.2233 - auc: 0.7928 - val_loss: 0.2183 - val_auc: 0.8315\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.21401\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.2068 - auc: 0.8297 - val_loss: 0.2000 - val_auc: 0.8633\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.21401 to 0.19996, saving model to output_path/1/model_1.h5\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1875 - auc: 0.8743 - val_loss: 0.1784 - val_auc: 0.8962\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.19996 to 0.17844, saving model to output_path/1/model_1.h5\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
            "376/376 [==============================] - 107s 284ms/step - loss: 0.1719 - auc: 0.8975 - val_loss: 0.1696 - val_auc: 0.9063\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.17844 to 0.16959, saving model to output_path/1/model_1.h5\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1564 - auc: 0.9160 - val_loss: 0.1671 - val_auc: 0.9145\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.16959 to 0.16710, saving model to output_path/1/model_1.h5\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1491 - auc: 0.9258 - val_loss: 0.1583 - val_auc: 0.9139\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.16710 to 0.15827, saving model to output_path/1/model_1.h5\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1383 - auc: 0.9356 - val_loss: 0.1557 - val_auc: 0.9226\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.15827 to 0.15574, saving model to output_path/1/model_1.h5\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.1239 - auc: 0.9481 - val_loss: 0.1605 - val_auc: 0.9232\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.15574\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1110 - auc: 0.9584 - val_loss: 0.1745 - val_auc: 0.9119\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.15574\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.34311412088573e-05.\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1081 - auc: 0.9603 - val_loss: 0.1636 - val_auc: 0.9146\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.15574\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.0935 - auc: 0.9656 - val_loss: 0.1688 - val_auc: 0.9176\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.15574\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.0832 - auc: 0.9731 - val_loss: 0.1814 - val_auc: 0.9104\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.15574\n",
            "Epoch 00016: early stopping\n",
            "\n",
            "Fold: 2\n",
            "train shape: 24066\n",
            "train steps_per_epoch: 376\n",
            "val shape: 6017\n",
            "validation steps_per_epoch: 94\n",
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "376/376 [==============================] - 688s 2s/step - loss: 0.7287 - auc: 0.5455 - val_loss: 0.5556 - val_auc: 0.5650\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.55558, saving model to output_path/2/model_2.h5\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 8.8e-05.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.3868 - auc: 0.5851 - val_loss: 0.2856 - val_auc: 0.7290\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.55558 to 0.28558, saving model to output_path/2/model_2.h5\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000166.\n",
            "376/376 [==============================] - 105s 280ms/step - loss: 0.2836 - auc: 0.6683 - val_loss: 0.2314 - val_auc: 0.7889\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.28558 to 0.23143, saving model to output_path/2/model_2.h5\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.000244.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.2402 - auc: 0.7407 - val_loss: 0.2191 - val_auc: 0.8232\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.23143 to 0.21913, saving model to output_path/2/model_2.h5\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.2200 - auc: 0.7917 - val_loss: 0.2004 - val_auc: 0.8471\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.21913 to 0.20039, saving model to output_path/2/model_2.h5\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.2080 - auc: 0.8232 - val_loss: 0.2559 - val_auc: 0.8582\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.20039\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1869 - auc: 0.8727 - val_loss: 0.1733 - val_auc: 0.8979\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.20039 to 0.17328, saving model to output_path/2/model_2.h5\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1680 - auc: 0.9028 - val_loss: 0.1723 - val_auc: 0.9028\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.17328 to 0.17227, saving model to output_path/2/model_2.h5\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.1567 - auc: 0.9117 - val_loss: 0.1632 - val_auc: 0.9243\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.17227 to 0.16325, saving model to output_path/2/model_2.h5\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1452 - auc: 0.9301 - val_loss: 0.1541 - val_auc: 0.9215\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.16325 to 0.15407, saving model to output_path/2/model_2.h5\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1321 - auc: 0.9421 - val_loss: 0.1582 - val_auc: 0.9190\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.15407\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1187 - auc: 0.9512 - val_loss: 0.1607 - val_auc: 0.9077\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.15407\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.978892583400011e-05.\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.1154 - auc: 0.9549 - val_loss: 0.1665 - val_auc: 0.9029\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.15407\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.0955 - auc: 0.9676 - val_loss: 0.1688 - val_auc: 0.9060\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.15407\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.0961 - auc: 0.9686 - val_loss: 0.1642 - val_auc: 0.9195\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.15407\n",
            "Epoch 00015: early stopping\n",
            "\n",
            "Fold: 3\n",
            "train shape: 24066\n",
            "train steps_per_epoch: 376\n",
            "val shape: 6017\n",
            "validation steps_per_epoch: 94\n",
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "376/376 [==============================] - 710s 2s/step - loss: 0.7622 - auc: 0.5371 - val_loss: 0.5835 - val_auc: 0.5334\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.58353, saving model to output_path/3/model_3.h5\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 8.8e-05.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.3969 - auc: 0.5682 - val_loss: 0.2877 - val_auc: 0.7283\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.58353 to 0.28772, saving model to output_path/3/model_3.h5\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000166.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.2873 - auc: 0.6455 - val_loss: 0.2306 - val_auc: 0.7863\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.28772 to 0.23061, saving model to output_path/3/model_3.h5\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.000244.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.2422 - auc: 0.7375 - val_loss: 0.2127 - val_auc: 0.8223\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.23061 to 0.21274, saving model to output_path/3/model_3.h5\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.2196 - auc: 0.7979 - val_loss: 0.2025 - val_auc: 0.8400\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.21274 to 0.20248, saving model to output_path/3/model_3.h5\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "376/376 [==============================] - 105s 281ms/step - loss: 0.2058 - auc: 0.8365 - val_loss: 0.1830 - val_auc: 0.8830\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.20248 to 0.18301, saving model to output_path/3/model_3.h5\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.1861 - auc: 0.8741 - val_loss: 0.1757 - val_auc: 0.8910\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.18301 to 0.17565, saving model to output_path/3/model_3.h5\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.1675 - auc: 0.9055 - val_loss: 0.1600 - val_auc: 0.9086\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.17565 to 0.16000, saving model to output_path/3/model_3.h5\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1565 - auc: 0.9103 - val_loss: 0.1662 - val_auc: 0.9216\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.16000\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1395 - auc: 0.9354 - val_loss: 0.1622 - val_auc: 0.9156\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.16000\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00013579520164057614.\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1361 - auc: 0.9344 - val_loss: 0.1598 - val_auc: 0.9239\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.16000 to 0.15981, saving model to output_path/3/model_3.h5\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1244 - auc: 0.9474 - val_loss: 0.1576 - val_auc: 0.9149\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.15981 to 0.15762, saving model to output_path/3/model_3.h5\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.1147 - auc: 0.9545 - val_loss: 0.1605 - val_auc: 0.9108\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.15762\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.0962 - auc: 0.9701 - val_loss: 0.1659 - val_auc: 0.9140\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.15762\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.0887 - auc: 0.9727 - val_loss: 0.1671 - val_auc: 0.9102\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.15762\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.0865 - auc: 0.9712 - val_loss: 0.1729 - val_auc: 0.9081\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.15762\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.150074382778258e-05.\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 4.3500744908800015e-05.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.0732 - auc: 0.9763 - val_loss: 0.1811 - val_auc: 0.9061\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.15762\n",
            "Epoch 00017: early stopping\n",
            "\n",
            "Fold: 4\n",
            "train shape: 24066\n",
            "train steps_per_epoch: 376\n",
            "val shape: 6017\n",
            "validation steps_per_epoch: 94\n",
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "376/376 [==============================] - 688s 2s/step - loss: 0.7167 - auc: 0.5424 - val_loss: 0.5478 - val_auc: 0.6428\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.54779, saving model to output_path/4/model_4.h5\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 8.8e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.3783 - auc: 0.5833 - val_loss: 0.2719 - val_auc: 0.7601\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.54779 to 0.27194, saving model to output_path/4/model_4.h5\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000166.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.2910 - auc: 0.6534 - val_loss: 0.2347 - val_auc: 0.7871\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.27194 to 0.23473, saving model to output_path/4/model_4.h5\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.000244.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.2467 - auc: 0.7242 - val_loss: 0.2185 - val_auc: 0.8201\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.23473 to 0.21847, saving model to output_path/4/model_4.h5\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.2232 - auc: 0.7805 - val_loss: 0.2038 - val_auc: 0.8483\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.21847 to 0.20381, saving model to output_path/4/model_4.h5\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.2054 - auc: 0.8338 - val_loss: 0.1979 - val_auc: 0.8736\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.20381 to 0.19793, saving model to output_path/4/model_4.h5\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.000322.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1873 - auc: 0.8722 - val_loss: 0.1803 - val_auc: 0.9014\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.19793 to 0.18028, saving model to output_path/4/model_4.h5\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
            "376/376 [==============================] - 107s 284ms/step - loss: 0.1702 - auc: 0.8964 - val_loss: 0.1686 - val_auc: 0.9074\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.18028 to 0.16855, saving model to output_path/4/model_4.h5\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1596 - auc: 0.9156 - val_loss: 0.1673 - val_auc: 0.9207\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.16855 to 0.16727, saving model to output_path/4/model_4.h5\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
            "376/376 [==============================] - 106s 281ms/step - loss: 0.1459 - auc: 0.9329 - val_loss: 0.1599 - val_auc: 0.9213\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.16727 to 0.15989, saving model to output_path/4/model_4.h5\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1303 - auc: 0.9427 - val_loss: 0.1702 - val_auc: 0.9253\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.15989\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
            "376/376 [==============================] - 107s 284ms/step - loss: 0.1193 - auc: 0.9517 - val_loss: 0.1668 - val_auc: 0.9116\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.15989\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.978892583400011e-05.\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.1149 - auc: 0.9541 - val_loss: 0.1591 - val_auc: 0.9171\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.15989 to 0.15908, saving model to output_path/4/model_4.h5\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.1052 - auc: 0.9597 - val_loss: 0.1749 - val_auc: 0.9220\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.15908\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.0889 - auc: 0.9699 - val_loss: 0.1733 - val_auc: 0.9171\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.15908\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.0813 - auc: 0.9783 - val_loss: 0.1743 - val_auc: 0.9198\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.15908\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 4.3500744908800015e-05.\n",
            "376/376 [==============================] - 106s 283ms/step - loss: 0.0806 - auc: 0.9707 - val_loss: 0.1770 - val_auc: 0.9211\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.15908\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 3.6800595927040014e-05.\n",
            "376/376 [==============================] - 106s 282ms/step - loss: 0.0741 - auc: 0.9808 - val_loss: 0.1815 - val_auc: 0.9077\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.15908\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.944047737400979e-05.\n",
            "Epoch 00018: early stopping\n",
            "Avoiding timeout...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-313dd30fd132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# To avoid timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Avoiding timeout...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "pbU435Gz32TW",
        "outputId": "9c6b8498-a8f3-4e05-91e4-2152f05b5932"
      },
      "source": [
        "from google.colab import files\r\n",
        "\r\n",
        "for fold in range(0, n_fold):\r\n",
        "  print(f\"downloading model_{fold}...\")\r\n",
        "  path = os.path.join(\"output_path\", str(fold))\r\n",
        "  mkdir_if_not_exist(path)\r\n",
        "  model_path = os.path.join(path, \"model_\"+str(fold)+\".h5\")\r\n",
        "  files.download(model_path) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading model_0...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_16e60a11-b8d3-41c6-afea-a868b1119e77\", \"model_0.h5\", 257013560)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "downloading model_1...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6744d071-81db-4acd-a2da-55d9792ada5d\", \"model_1.h5\", 257013560)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "downloading model_2...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_45044d68-163d-4ede-ad0a-c461075ec15d\", \"model_2.h5\", 257013560)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "downloading model_3...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4df0ca3e-1a34-4ab7-9719-af29f1dc405e\", \"model_3.h5\", 257013560)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "downloading model_4...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e8ec6048-c898-48a9-a3c0-e8b274ea57a4\", \"model_4.h5\", 257013560)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O6Frym0Y7ad"
      },
      "source": [
        "### Ensemble predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C9NmGUoVBvav",
        "outputId": "286177fc-16f3-4649-f151-7faac5e744d3"
      },
      "source": [
        "test_paths = GCS_DS_PATH + \"test/\" + df_sub['StudyInstanceUID'] + '.jpg'\r\n",
        "test_paths[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gs://kds-26cc3b044601ad8c96f3938b262de2a183d1084c40612a7f86ddf418/test/1.2.826.0.1.3680043.8.498.46923145579096002617106567297135160932.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "spYtDp-mW_7C",
        "outputId": "241ab57e-983d-47ff-957c-7e05674ced07"
      },
      "source": [
        "test_decoder = build_decoder(with_labels=False, target_size=(img_size, img_size))\r\n",
        "\r\n",
        "dtest = build_dataset(\r\n",
        "    test_paths[0:test_limit], bsize=batch_size, repeat=False, \r\n",
        "    shuffle=False, augment=False, cache=False, \r\n",
        "    decode_fn=test_decoder\r\n",
        ")\r\n",
        "\r\n",
        "ensemble_preds = np.zeros((len(test_paths[0:test_limit]), n_classes), dtype=np.float)\r\n",
        "\r\n",
        "for fold in range(0, n_fold):\r\n",
        "  print(f\"model: {fold}\")\r\n",
        "  path = os.path.join(\"output_path\", str(fold))\r\n",
        "  mkdir_if_not_exist(path)\r\n",
        "  model_path = os.path.join(path, \"model_\"+str(fold)+\".h5\")\r\n",
        "  \r\n",
        "  with strategy.scope():\r\n",
        "      model = tf.keras.models.load_model(model_path)\r\n",
        "\r\n",
        "      y_preds = model.predict(dtest, verbose=1)\r\n",
        "\r\n",
        "      # sum the predicted values\r\n",
        "      ensemble_preds += np.array(y_preds, dtype=np.float)\r\n",
        "\r\n",
        "# average the predicted values\r\n",
        "ensemble_preds /= n_fold\r\n",
        "df_sub.iloc[:, 1:] = ensemble_preds\r\n",
        "display(df_sub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model: 0\n",
            "56/56 [==============================] - 82s 1s/step\n",
            "model: 1\n",
            "56/56 [==============================] - 81s 1s/step\n",
            "model: 2\n",
            "56/56 [==============================] - 83s 1s/step\n",
            "model: 3\n",
            "56/56 [==============================] - 84s 1s/step\n",
            "model: 4\n",
            "56/56 [==============================] - 84s 1s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudyInstanceUID</th>\n",
              "      <th>ETT - Abnormal</th>\n",
              "      <th>ETT - Borderline</th>\n",
              "      <th>ETT - Normal</th>\n",
              "      <th>NGT - Abnormal</th>\n",
              "      <th>NGT - Borderline</th>\n",
              "      <th>NGT - Incompletely Imaged</th>\n",
              "      <th>NGT - Normal</th>\n",
              "      <th>CVC - Abnormal</th>\n",
              "      <th>CVC - Borderline</th>\n",
              "      <th>CVC - Normal</th>\n",
              "      <th>Swan Ganz Catheter Present</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.46923145579096002617...</td>\n",
              "      <td>0.003980</td>\n",
              "      <td>0.148109</td>\n",
              "      <td>0.751670</td>\n",
              "      <td>0.006566</td>\n",
              "      <td>0.004812</td>\n",
              "      <td>0.017220</td>\n",
              "      <td>0.967534</td>\n",
              "      <td>0.034764</td>\n",
              "      <td>0.143648</td>\n",
              "      <td>0.961513</td>\n",
              "      <td>0.997017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.84006870182611080091...</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.006026</td>\n",
              "      <td>0.005555</td>\n",
              "      <td>0.994002</td>\n",
              "      <td>0.000118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.12219033294413119947...</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.028151</td>\n",
              "      <td>0.392895</td>\n",
              "      <td>0.643898</td>\n",
              "      <td>0.000217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.84994474380235968109...</td>\n",
              "      <td>0.003846</td>\n",
              "      <td>0.029073</td>\n",
              "      <td>0.049871</td>\n",
              "      <td>0.018470</td>\n",
              "      <td>0.013193</td>\n",
              "      <td>0.957945</td>\n",
              "      <td>0.039246</td>\n",
              "      <td>0.113185</td>\n",
              "      <td>0.254475</td>\n",
              "      <td>0.611831</td>\n",
              "      <td>0.019305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.35798987793805669662...</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.002610</td>\n",
              "      <td>0.050831</td>\n",
              "      <td>0.237322</td>\n",
              "      <td>0.804757</td>\n",
              "      <td>0.000424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3577</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.81464483108873296584...</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.001749</td>\n",
              "      <td>0.004719</td>\n",
              "      <td>0.008885</td>\n",
              "      <td>0.017725</td>\n",
              "      <td>0.926567</td>\n",
              "      <td>0.029308</td>\n",
              "      <td>0.006811</td>\n",
              "      <td>0.090162</td>\n",
              "      <td>0.958868</td>\n",
              "      <td>0.001191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3578</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.33579133018211530710...</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>0.002221</td>\n",
              "      <td>0.001579</td>\n",
              "      <td>0.003761</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.590587</td>\n",
              "      <td>0.351006</td>\n",
              "      <td>0.168774</td>\n",
              "      <td>0.001190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3579</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.61472811086105902907...</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.006323</td>\n",
              "      <td>0.024265</td>\n",
              "      <td>0.967607</td>\n",
              "      <td>0.000125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3580</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.19434375795525494655...</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000629</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.062418</td>\n",
              "      <td>0.274576</td>\n",
              "      <td>0.631775</td>\n",
              "      <td>0.000148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3581</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.21182456828145534541...</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.010997</td>\n",
              "      <td>0.016590</td>\n",
              "      <td>0.986736</td>\n",
              "      <td>0.000191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3582 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       StudyInstanceUID  ...  Swan Ganz Catheter Present\n",
              "0     1.2.826.0.1.3680043.8.498.46923145579096002617...  ...                    0.997017\n",
              "1     1.2.826.0.1.3680043.8.498.84006870182611080091...  ...                    0.000118\n",
              "2     1.2.826.0.1.3680043.8.498.12219033294413119947...  ...                    0.000217\n",
              "3     1.2.826.0.1.3680043.8.498.84994474380235968109...  ...                    0.019305\n",
              "4     1.2.826.0.1.3680043.8.498.35798987793805669662...  ...                    0.000424\n",
              "...                                                 ...  ...                         ...\n",
              "3577  1.2.826.0.1.3680043.8.498.81464483108873296584...  ...                    0.001191\n",
              "3578  1.2.826.0.1.3680043.8.498.33579133018211530710...  ...                    0.001190\n",
              "3579  1.2.826.0.1.3680043.8.498.61472811086105902907...  ...                    0.000125\n",
              "3580  1.2.826.0.1.3680043.8.498.19434375795525494655...  ...                    0.000148\n",
              "3581  1.2.826.0.1.3680043.8.498.21182456828145534541...  ...                    0.000191\n",
              "\n",
              "[3582 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au14_jpipx9e"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "1.   [RANZCR CLiP - Catheter and Line Position Challenge](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification)\n",
        "2.   [CoroNet: A deep neural network for detection and diagnosis of COVID-19 from chest x-ray images](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7274128/)\n",
        "3.   [RANZCR TF RECORDS 768 STRATIFIED](https://www.kaggle.com/ragnar123/ranzcr-tf-records-768-stratified/code)\n",
        "\n",
        "\n"
      ]
    }
  ]
}